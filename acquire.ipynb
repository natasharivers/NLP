{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Make a function that works on a single url\n",
    "# Make sure your function has everything it needs inside (try to avoid globals)\n",
    "\n",
    "def get_codeup_blog(url):\n",
    "    \n",
    "    # Set the headers to show as Netscape Navigator on Windows 98, b/c I feel like creating an anomaly in the logs\n",
    "    headers = {\"User-Agent\": \"Mozilla/4.5 (compatible; HTTrack 3.0x; Windows 98)\"}\n",
    "\n",
    "    # Get the http response object from the server\n",
    "    response = get(url, headers=headers)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    title = soup.find(\"h1\").text\n",
    "    published_date = soup.time.text\n",
    "    \n",
    "    if len(soup.select(\".jupiterx-post-image\")) > 0:\n",
    "        blog_image = soup.select(\".jupiterx-post-image\")[0].picture.img[\"data-src\"]\n",
    "    else:\n",
    "        blog_image = None\n",
    "        \n",
    "    content = soup.select(\".jupiterx-post-content\")[0].text\n",
    "    \n",
    "    output = {}\n",
    "    output[\"title\"] = title\n",
    "    output[\"published_date\"] = published_date\n",
    "    output[\"blog_image\"] = blog_image\n",
    "    output[\"content\"] = content\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "url = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/',\n",
    "        'https://codeup.com/data-science-myths/',\n",
    "        'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/',\n",
    "        'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/',\n",
    "        'https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']\n",
    "\n",
    "def get_website_info(urls):\n",
    "    '''takes in a list of urls provided by the user\n",
    "    creates and empty dictionary to hold upcoming info\n",
    "    loop through the list of urls and\n",
    "        scrape the webpage\n",
    "        create a tasty little soup\n",
    "        gather webpage information\n",
    "        create title and content that will hold info from each url\n",
    "            title takes in title of the webpage\n",
    "            content hold all other info\n",
    "    add title and content to the empty dictonary\n",
    "    turn the dictionary into a data frame\n",
    "    return said dataframe'''\n",
    "    # set an empty dictionary to start\n",
    "    dictionary = []\n",
    "    # loops through list of urls\n",
    "    for url in urls:\n",
    "        # make header to gain access to webpage\n",
    "        headers = {'User-Agent': 'Codeup Data Science'}\n",
    "        # scrape the webpaage\n",
    "        response = get(url, headers=headers)\n",
    "        # takes URL and returns the soup object\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        # find the full webpage info\n",
    "        website = soup.find('div', class_='jupiterx-post-content')\n",
    "        # create title and content that will hold the info\n",
    "        website_info = {'title':[], 'content':[]}\n",
    "        # adds title to webpage info\n",
    "        website_info['title'] = soup.title.string\n",
    "        # adds websites content to webpag info\n",
    "        website_info['content'] = website.text\n",
    "        # adds this dict of the website to the webpage info\n",
    "        dictionary.append(website_info)\n",
    "    # turn it into a dataframe\n",
    "    dictionary = pd.DataFrame(dictionary)\n",
    "    # return the new df\n",
    "    return dictionary\n",
    "\n",
    "def get_blog_articles(urls):\n",
    "    # List of dictionaries\n",
    "    posts = [get_codeup_blog(url) for url in urls]\n",
    "    \n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "\n",
    "def acquire_codeup_blog():\n",
    "    urls = [\n",
    "        \"https://codeup.com/codeups-data-science-career-accelerator-is-here/\",\n",
    "        \"https://codeup.com/data-science-myths/\",\n",
    "        \"https://codeup.com/data-science-vs-data-analytics-whats-the-difference/\",\n",
    "        \"https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/\",\n",
    "        \"https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/\"\n",
    "    ]\n",
    "\n",
    "    return get_blog_articles(urls)\n",
    "\n",
    "\n",
    "\n",
    "# Inshort article\n",
    "def get_article(article, category):\n",
    "    # Attribute selector\n",
    "    title = article.select(\"[itemprop='headline']\")[0].text\n",
    "    \n",
    "    # article body\n",
    "    content = article.select(\"[itemprop='articleBody']\")[0].text\n",
    "    \n",
    "    output = {}\n",
    "    output[\"title\"] = title\n",
    "    output[\"content\"] = content\n",
    "    output[\"category\"] = category\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_articles(category):\n",
    "    \"\"\"\n",
    "    This function takes in a category as a string. Category must be an available category in inshorts\n",
    "    Returns a list of dictionaries where each dictionary represents a single inshort article\n",
    "    \"\"\"\n",
    "    base = \"https://inshorts.com/en/read/\"\n",
    "    \n",
    "    # We concatenate our base_url with the category\n",
    "    url = base + category\n",
    "    \n",
    "    # Set the headers to show as Netscape Navigator on Windows 98, b/c I feel like creating an anomaly in the logs\n",
    "    headers = {\"User-Agent\": \"Mozilla/4.5 (compatible; HTTrack 3.0x; Windows 98)\"}\n",
    "\n",
    "    # Get the http response object from the server\n",
    "    response = get(url, headers=headers)\n",
    "\n",
    "    # Make soup out of the raw html\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # Ignore everything, focusing only on the news cards\n",
    "    articles = soup.select(\".news-card\")\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    # Iterate through every article tag/soup \n",
    "    for article in articles:\n",
    "        \n",
    "        # Returns a dictionary of the article's title, body, and category\n",
    "        article_data = get_article(article, category) \n",
    "        \n",
    "        # Append the dictionary to the list\n",
    "        output.append(article_data)\n",
    "    \n",
    "    # Return the list of dictionaries\n",
    "    return output\n",
    "    \n",
    "\n",
    "def get_all_news_articles(categories):\n",
    "    \"\"\"\n",
    "    Takes in a list of categories where the category is part of the URL pattern on inshorts\n",
    "    Returns a dataframe of every article from every category listed\n",
    "    Each row in the dataframe is a single article\n",
    "    \"\"\"\n",
    "    all_inshorts = []\n",
    "\n",
    "    for category in categories:\n",
    "        all_category_articles = get_articles(category)\n",
    "        all_inshorts = all_inshorts + all_category_articles\n",
    "\n",
    "    df = pd.DataFrame(all_inshorts)\n",
    "    return df\n",
    "\n",
    "\n",
    "def acquire_news_articles():\n",
    "    categories = [\"business\", \"sports\", \"technology\", \"entertainment\", \"science\", \"world\"]\n",
    "    return get_all_news_articles(categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
