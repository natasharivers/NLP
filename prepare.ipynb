{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "    - Lowercase everything\n",
    "    - Normalize unicode characters\n",
    "    - Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "<br>\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "<br>\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "<br>\n",
    "\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "    - This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "<br>\n",
    "\n",
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "<br>\n",
    "\n",
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "<br>\n",
    "\n",
    "8. For each dataframe, produce the following columns:\n",
    "\n",
    "    - title to hold the title\n",
    "    - original to hold the original article/post content\n",
    "    - clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "    - stemmed to hold the stemmed version of the cleaned data.\n",
    "    - lemmatized to hold the lemmatized version of the cleaned data.\n",
    "<br>\n",
    "\n",
    "9. Ask yourself:\n",
    "\n",
    "    - If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    - If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    - If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natasharivers/codeup-data-science/database-exercises/NLP/acquire.py:92: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 92 of the file /Users/natasharivers/codeup-data-science/database-exercises/NLP/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    }
   ],
   "source": [
    "# define categories\n",
    "categories = [\"business\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "# use get_all_new_article function from acquire.py file \n",
    "\n",
    "news_df = acquire.get_all_news_articles(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reliance Industries vaccinates 98% of workers,...</td>\n",
       "      <td>Reliance Industries has said in a statement th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will most likely not be on future earnings c...</td>\n",
       "      <td>Tesla CEO and the world's second-richest perso...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speculation around our plans for crypto not tr...</td>\n",
       "      <td>Amazon on Monday denied speculations that it w...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Musk criticises Apple's 'walled garden', cobal...</td>\n",
       "      <td>Tesla's billionaire CEO Elon Musk criticised A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Factually incorrect: INOX on report of Amazon ...</td>\n",
       "      <td>INOX Leisure denied a report that claimed Amaz...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Reliance Industries vaccinates 98% of workers,...   \n",
       "1  I will most likely not be on future earnings c...   \n",
       "2  Speculation around our plans for crypto not tr...   \n",
       "3  Musk criticises Apple's 'walled garden', cobal...   \n",
       "4  Factually incorrect: INOX on report of Amazon ...   \n",
       "\n",
       "                                             content  category  \n",
       "0  Reliance Industries has said in a statement th...  business  \n",
       "1  Tesla CEO and the world's second-richest perso...  business  \n",
       "2  Amazon on Monday denied speculations that it w...  business  \n",
       "3  Tesla's billionaire CEO Elon Musk criticised A...  business  \n",
       "4  INOX Leisure denied a report that claimed Amaz...  business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the head of dataframe\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reliance Industries has said in a statement that over 98% of its workers have received at least one dose of COVID-19 vaccine so far. The billionaire Mukesh Ambani-led conglomerate had over 2.36 lakh employees, of March 31. Besides Reliance, Hindustan Unilever has also given at least one shot to 90% of employees, while Infosys inoculated 59% employees and TCS 70%.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets use the content of first news item as 'article' to test my functions\n",
    "article = news_df.content[0]\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1. Define a function named basic_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in the original text.\n",
    "    The text is all lowercased and any characters that are not ascii are ignored\n",
    "    additionally, special characters are all removed.\n",
    "    A clean article is then returned\n",
    "    '''\n",
    "    #lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    #normalize\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    \n",
    "    #remove special characters and replaces it with blank\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reliance industries has said in a statement that over 98 of its workers have received at least one dose of covid19 vaccine so far the billionaire mukesh ambaniled conglomerate had over 236 lakh employees of march 31 besides reliance hindustan unilever has also given at least one shot to 90 of employees while infosys inoculated 59 employees and tcs 70'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure function works\n",
    "basic_clean(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2. Define a function named tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string\n",
    "    and returns the string as individual tokens put back into the string\n",
    "    '''\n",
    "    #create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    #use the tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reliance Industries has said in a statement that over 98 % of its workers have received at least one dose of COVID-19 vaccine so far. The billionaire Mukesh Ambani-led conglomerate had over 2.36 lakh employees , of March 31. Besides Reliance , Hindustan Unilever has also given at least one shot to 90 % of employees , while Infosys inoculated 59 % employees and TCS 70 % .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure function works\n",
    "tokenize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3. Define a function named stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''\n",
    "    This function takes in text\n",
    "    and returns the stem word joined back into the text\n",
    "    '''\n",
    "    #create porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    #use the stem\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    #join stem word to string\n",
    "    text_stemmed = ' '.join(stems)\n",
    "\n",
    "    return text_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relianc industri ha said in a statement that over 98% of it worker have receiv at least one dose of covid-19 vaccin so far. the billionair mukesh ambani-l conglomer had over 2.36 lakh employees, of march 31. besid reliance, hindustan unilev ha also given at least one shot to 90% of employees, while infosi inocul 59% employe and tc 70%.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure function works\n",
    "stem(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4. Define a function named lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function takes in text\n",
    "    and returns the lemmatized word joined back into the text\n",
    "    '''\n",
    "    #create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    #look at the article \n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "    \n",
    "    #join lemmatized words into article\n",
    "    text_lemmatized= ' '.join(lemmas)\n",
    "\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reliance Industries ha said in a statement that over 98% of it worker have received at least one dose of COVID-19 vaccine so far. The billionaire Mukesh Ambani-led conglomerate had over 2.36 lakh employees, of March 31. Besides Reliance, Hindustan Unilever ha also given at least one shot to 90% of employees, while Infosys inoculated 59% employee and TCS 70%.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure function works\n",
    "lemmatize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #5. Define a function named remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in text, extra words and exclude words\n",
    "    and returns a list of text with stopword removed\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reliance Industries said statement 98% workers received least one dose COVID-19 vaccine far. The billionaire Mukesh Ambani-led conglomerate 2.36 lakh employees, March 31. Besides Reliance, Hindustan Unilever also given least one shot 90% employees, Infosys inoculated 59% employees TCS 70%.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reliance Industries vaccinates 98% of workers,...</td>\n",
       "      <td>Reliance Industries has said in a statement th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will most likely not be on future earnings c...</td>\n",
       "      <td>Tesla CEO and the world's second-richest perso...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speculation around our plans for crypto not tr...</td>\n",
       "      <td>Amazon on Monday denied speculations that it w...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Musk criticises Apple's 'walled garden', cobal...</td>\n",
       "      <td>Tesla's billionaire CEO Elon Musk criticised A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Factually incorrect: INOX on report of Amazon ...</td>\n",
       "      <td>INOX Leisure denied a report that claimed Amaz...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Reliance Industries vaccinates 98% of workers,...   \n",
       "1  I will most likely not be on future earnings c...   \n",
       "2  Speculation around our plans for crypto not tr...   \n",
       "3  Musk criticises Apple's 'walled garden', cobal...   \n",
       "4  Factually incorrect: INOX on report of Amazon ...   \n",
       "\n",
       "                                             content  category  \n",
       "0  Reliance Industries has said in a statement th...  business  \n",
       "1  Tesla CEO and the world's second-richest perso...  business  \n",
       "2  Amazon on Monday denied speculations that it w...  business  \n",
       "3  Tesla's billionaire CEO Elon Musk criticised A...  business  \n",
       "4  INOX Leisure denied a report that claimed Amaz...  business  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at news_ddf\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     reliance industry ha said statement 98 worker ...\n",
       "1     tesla ceo world ' secondrichest person elon mu...\n",
       "2     amazon monday denied speculation wa looking ac...\n",
       "3     tesla ' billionaire ceo elon musk criticised a...\n",
       "4     inox leisure denied report claimed amazon indi...\n",
       "                            ...                        \n",
       "93    marathi actor umesh kamat ha issued statement ...\n",
       "94    veteran actress savita bajaj unwell facing fin...\n",
       "95    shefali shah speaking international emmy award...\n",
       "96    taking instagram story singer aditya narayan d...\n",
       "97    actor adil hussain feel bollywood rope actor n...\n",
       "Name: content, Length: 98, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply all functions just created to the content column\n",
    "news_df['content'].apply(basic_clean)\\\n",
    ".apply(tokenize)\\\n",
    ".apply(lemmatize)\\\n",
    ".apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natasharivers/codeup-data-science/database-exercises/NLP/acquire.py:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file /Users/natasharivers/codeup-data-science/database-exercises/NLP/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    }
   ],
   "source": [
    "#bring in blogs using acquire.py\n",
    "codeup_df = acquire.acquire_codeup_blog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog_image</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      published_date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                          blog_image  \\\n",
       "0  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "1  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "2  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at the material\n",
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rumor true time ha arrived codeup ha officiall...\n",
       "1    dimitri antoniou maggie giust data science big...\n",
       "2    dimitri antoniou week ago codeup launched imme...\n",
       "3    sa tech job fair third biannual san antonio te...\n",
       "4    competitor bootcamps closing model danger prog...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply all functions just created to the content column\n",
    "codeup_df['content'].apply(basic_clean)\\\n",
    ".apply(tokenize)\\\n",
    ".apply(lemmatize)\\\n",
    ".apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #8. For each dataframe, make columns: title, original, clean, stemmed, lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ PREP ARTICLES ################################\n",
    "\n",
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #9. Ask yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize- smaller dataset, it is slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming- larger dataset, it is faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
